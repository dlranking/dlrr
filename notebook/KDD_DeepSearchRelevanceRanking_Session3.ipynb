{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDD_2022_tutorial_session3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "This notebook will walk you through the basic steps for knowledge distillation, using simple teacher and student models implemented from [Hugging Face Transformer](https://huggingface.co/docs/transformers/index).We will also conduct experiments on the QADSM task in [xGLUE](https://huggingface.co/datasets/xglue) dataset, which is extracted from real Bing Ads traffic. "
      ],
      "metadata": {
        "id": "RCHRwy30Kg4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparations for Colab\n",
        "\n",
        "Running the code snippets in this notebook requires a GPU runtime, and we also need to install some dependencies.\n"
      ],
      "metadata": {
        "id": "3wOHwbcmcWDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/KDD_2022/\n",
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "# test whether we can import packages properly\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "nTriABf_SSSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code used in this notebook are available in https://github.com/sufferandjoy/kdd_2022_tutorial.git:"
      ],
      "metadata": {
        "id": "xmG1lH31dPrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sufferandjoy/kdd_2022_tutorial.git"
      ],
      "metadata": {
        "id": "SiybH5wsdELS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teacher and Student Model\n",
        "\n",
        "As mentioned above, both our teacher model and student model are implemented using Hugging Face Tranformer. More specifically, we use [BERT-Mini](https://huggingface.co/google/bert_uncased_L-4_H-256_A-4) (4 layers, 4 attention heads, hidden layer size 256) as our teacher model, and a [TwinBERT](https://arxiv.org/abs/2002.06275) model constructed from two [BERT-Tiny](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) (2 layers, 2 attention heads, hidden layer size 128) as student model. Other model structures are also supported, which could be set by the `teacher_pretrained` and `student_pretrained` parameter. Below are the model strcture specified in `model.py`:"
      ],
      "metadata": {
        "id": "-qKZi5dPdfsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.model = BertModel.from_pretrained(args.teacher_pretrained)\n",
        "        hidden_size = int(args.teacher_pretrained.split('/')[1].split('_')[3].split('-')[-1])\n",
        "        self.ff = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        bert_output = self.model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        output = torch.sigmoid(self.ff(bert_output.pooler_output))\n",
        "        return output\n",
        "\n",
        "\n",
        "class TwinBERT(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(TwinBERT, self).__init__()\n",
        "        self.encoder_model = BertModel.from_pretrained(args.student_pretrained)\n",
        "\n",
        "    def forward(self, seq1, mask1, seq2, mask2):\n",
        "        output_1 = self.encoder_model(seq1, attention_mask=mask1).pooler_output\n",
        "        output_2 = self.encoder_model(seq2, attention_mask=mask2).pooler_output\n",
        "        cosine_similarity = nn.functional.cosine_similarity(output_1, output_2).unsqueeze(-1)\n",
        "        return cosine_similarity"
      ],
      "metadata": {
        "id": "LS1_aPhrfdm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TwinBERT model has a two-tower structure, implemented as two BERT encoders sharing the same weights, as shown in the figure below. In this notebook, we will use query as input to the left encoder, and the concatenation of ad_title and ad_description as input to the other encoder.\n",
        "\n",
        "![twinbert.png](https://drive.google.com/uc?id=1H3qpUI8LwqOKnk9NWbf7s6cY04SlbtED)\n"
      ],
      "metadata": {
        "id": "RuoK0T92gKVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Preprocessing\n",
        "\n",
        "Below we show several samples from the QADSM task in XGLUE, where each sample contains `query`, `ad_title`, `ad_description`, and a binary label named `relevance_label` indicating the relevance between each query-ad pair. In this notebook we will conduct training on the 100K `train` split, and conduct evaluation on the 10K `test.en` split. We further remove all training and test samples starting with \"ERROR_AdRejected\".\n",
        "\n",
        "![QADSM task in XGLUE](https://drive.google.com/uc?id=1TQDw1b5iZeonUONcbdutO-7XkmFhwEva)\n",
        "\n",
        "The logic for loading this dataset and pre-processing samples are implemented in `utils.py`. Because our teacher model and student model have different input schema (teacher takes as input a single text sequence while student takes as input two), we implement two different preprocess functions as below. Note that in `preprocess_function_student()`, we will concatenate `ad_title` and `ad_description` as the ad text, and the output will contain tokenized results for both query and ad text."
      ],
      "metadata": {
        "id": "k84Hutflf5J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # Concatenate ad_title and ad_description\n",
        "    texts = []\n",
        "    for i in range(len(examples['query'])):\n",
        "        new_text = (examples['query'][i], examples['ad_title'][i] + ' ' + examples['ad_description'][i])\n",
        "        texts.append(new_text)\n",
        "\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=args.max_length,\n",
        "        return_special_tokens_mask=False,\n",
        "    )\n",
        "\n",
        "def preprocess_function_student(examples):\n",
        "    # Concatenate ad_title and ad_description\n",
        "    texts = []\n",
        "    for i in range(len(examples['query'])):\n",
        "        new_text = examples['ad_title'][i] + ' ' + examples['ad_description'][i]\n",
        "        texts.append(new_text)\n",
        "\n",
        "    tok_q = tokenizer(\n",
        "        examples['query'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=args.max_length_query,\n",
        "        return_special_tokens_mask=False,\n",
        "    )\n",
        "\n",
        "    tok_a = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=args.max_length_ad,\n",
        "        return_special_tokens_mask=False,\n",
        "    )\n",
        "    tok_q['input_ids_2'] = tok_a['input_ids']\n",
        "    tok_q['attention_mask_2'] = tok_a['attention_mask']\n",
        "    tok_q['token_type_ids_2'] = tok_a['token_type_ids']\n",
        "    return tok_q\n",
        "\n",
        "# process dataset\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function if args.model == 'teacher' else preprocess_function_student,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    # remove_columns=dataset[\"train\"].column_names,\n",
        "    load_from_cache_file=True,\n",
        "    # desc=\"Running tokenizer on dataset line_by_line\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZOfmoBvjqOis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Experiment on Knowledge Distillation\n",
        "\n",
        "Having introduced both models and dataset, next we will show you the major steps in knowledge distillation and demonstrate its effectiveness. To begin with, we will first introduce a key parameter in our code called `task`, which supports five different settings:\n",
        "\n",
        "\n",
        "*   `teacher_ft`: this is the setting that allows us to load a pretrained teacher model and finetune it on the binary labels in QADSM task.\n",
        "*   `student_ft`: similarly, this is the setting to finetune our student model directly on the binary labels.\n",
        "*   `teacher_inf`: this setting allows us to conduct inference using our best finetuned teacher model, where \"best\" means having the smallest validation loss.\n",
        "*   `student_kd`: this setting allows us to train our student model by regression to the teacher model obtained in `teacher_inf` setting.\n",
        "*   `eval`: this setting will do a full evaluation on the same test data, to compare the performance of `teacher_ft`, `student_ft` and `student_kd`.\n",
        "\n",
        "### Step 0: Student Finetuning on Binary Labels as Baseline\n",
        "\n",
        "We firstly run our code using the `student_ft` setting, in order to get an idea on how well we are doing without knowledge distillation:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFZ916dLrTqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task student_ft --train_batch_size 512 --val_batch_size 2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CSXy0Dgdh8Q",
        "outputId": "a660d47b-d6c1-4ab3-efbe-2d1996c76f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments\n",
            "-- Argument: device                     -- cuda\n",
            "-- Argument: learning_rate              -- 0.0001\n",
            "-- Argument: load_dataset_py_path       -- load_dataset.py\n",
            "-- Argument: logfreq                    -- 100\n",
            "-- Argument: max_length                 -- 32\n",
            "-- Argument: max_length_ad              -- 24\n",
            "-- Argument: max_length_query           -- 9\n",
            "-- Argument: num_epochs                 -- 10\n",
            "-- Argument: output_dir                 -- output/student_ft\n",
            "-- Argument: student_pretrained         -- google/bert_uncased_L-2_H-128_A-2\n",
            "-- Argument: task                       -- student_ft\n",
            "-- Argument: teacher_pretrained         -- google/bert_uncased_L-4_H-256_A-4\n",
            "-- Argument: train_batch_size           -- 512\n",
            "-- Argument: val_batch_size             -- 2048\n",
            "Reusing dataset xglue (/root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08)\n",
            "\r  0% 0/7 [00:00<?, ?it/s]\r100% 7/7 [00:00<00:00, 1037.13it/s]\n",
            "Load labeled dataset.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-5595ab17a87c5d28.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-fada6ddeda8b2bf9.arrow\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Load Student model (TwinBERT) google/bert_uncased_L-2_H-128_A-2.\n",
            "Parameter 'function'=<function prepare_data_loader.<locals>.preprocess_function_student at 0x7f629acfda70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 98/98 [00:10<00:00,  9.07ba/s]\n",
            "100% 10/10 [00:01<00:00,  5.13ba/s]\n",
            "Training started.\n",
            "-- Epoch 0, Step: 99, Avg Loss = 0.263883\n",
            "-- Epoch 0: PR AUC 0.556535, ROC AUC 0.574887, Validation Loss 0.247066\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 1, Step: 99, Avg Loss = 0.244357\n",
            "-- Epoch 1: PR AUC 0.603204, ROC AUC 0.617511, Validation Loss 0.239404\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 2, Step: 99, Avg Loss = 0.232825\n",
            "-- Epoch 2: PR AUC 0.616062, ROC AUC 0.633487, Validation Loss 0.237377\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 3, Step: 99, Avg Loss = 0.222213\n",
            "-- Epoch 3: PR AUC 0.633256, ROC AUC 0.646686, Validation Loss 0.236590\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 4, Step: 99, Avg Loss = 0.210781\n",
            "-- Epoch 4: PR AUC 0.636228, ROC AUC 0.650406, Validation Loss 0.235875\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 5, Step: 99, Avg Loss = 0.203266\n",
            "-- Epoch 5: PR AUC 0.637875, ROC AUC 0.655060, Validation Loss 0.236999\n",
            "-- Epoch 6, Step: 99, Avg Loss = 0.197157\n",
            "-- Epoch 6: PR AUC 0.639070, ROC AUC 0.658543, Validation Loss 0.236648\n",
            "-- Epoch 7, Step: 99, Avg Loss = 0.188948\n",
            "-- Epoch 7: PR AUC 0.636729, ROC AUC 0.655552, Validation Loss 0.238788\n",
            "-- Epoch 8, Step: 99, Avg Loss = 0.183726\n",
            "-- Epoch 8: PR AUC 0.633846, ROC AUC 0.650599, Validation Loss 0.241287\n",
            "-- Epoch 9, Step: 99, Avg Loss = 0.179440\n",
            "-- Epoch 9: PR AUC 0.633994, ROC AUC 0.652230, Validation Loss 0.242440\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Teacher Finetuning\n",
        "\n",
        "Having obtained the above baseline, we then turn to the first step in knowledge distillation which is teacher finetuning. Typical teacher models used in industrial applications are usually very powerful and hence very resource-consuming, but thanks to knowldege distillation, we do not need to worry about how to serve such models online. Instead, all the training and inference jobs running on these models would happen offline only, and it is the light-weight student model that would be deployed to the online environment. \n",
        "\n",
        "In this notebook, we can run the following code snippet for teacher finetuning, similar to what we did in the last step:"
      ],
      "metadata": {
        "id": "-fWzvrIpYqZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task teacher_ft --train_batch_size 512 --val_batch_size 2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a39AbzejAKEA",
        "outputId": "8bbddb1c-c023-4444-bd35-718d49ff16fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments\n",
            "-- Argument: device                     -- cuda\n",
            "-- Argument: learning_rate              -- 0.0001\n",
            "-- Argument: load_dataset_py_path       -- load_dataset.py\n",
            "-- Argument: logfreq                    -- 100\n",
            "-- Argument: max_length                 -- 32\n",
            "-- Argument: max_length_ad              -- 24\n",
            "-- Argument: max_length_query           -- 9\n",
            "-- Argument: num_epochs                 -- 10\n",
            "-- Argument: output_dir                 -- output/teacher_ft\n",
            "-- Argument: student_pretrained         -- google/bert_uncased_L-2_H-128_A-2\n",
            "-- Argument: task                       -- teacher_ft\n",
            "-- Argument: teacher_pretrained         -- google/bert_uncased_L-4_H-256_A-4\n",
            "-- Argument: train_batch_size           -- 512\n",
            "-- Argument: val_batch_size             -- 2048\n",
            "Reusing dataset xglue (/root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08)\n",
            "100% 7/7 [00:00<00:00, 1016.03it/s]\n",
            "Load labeled dataset.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-5595ab17a87c5d28.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-fada6ddeda8b2bf9.arrow\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Load Teacher model google/bert_uncased_L-4_H-256_A-4.\n",
            "Parameter 'function'=<function prepare_data_loader.<locals>.preprocess_function at 0x7f414c02cef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 98/98 [00:09<00:00, 10.79ba/s]\n",
            "100% 10/10 [00:00<00:00, 10.65ba/s]\n",
            "Training started.\n",
            "-- Epoch 0, Step: 99, Avg Loss = 0.242966\n",
            "-- Epoch 0: PR AUC 0.682490, ROC AUC 0.692977, Validation Loss 0.225106\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 1, Step: 99, Avg Loss = 0.220504\n",
            "-- Epoch 1: PR AUC 0.697206, ROC AUC 0.710138, Validation Loss 0.218986\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 2, Step: 99, Avg Loss = 0.202760\n",
            "-- Epoch 2: PR AUC 0.709385, ROC AUC 0.719942, Validation Loss 0.214689\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 3, Step: 99, Avg Loss = 0.191179\n",
            "-- Epoch 3: PR AUC 0.716551, ROC AUC 0.727958, Validation Loss 0.213000\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 4, Step: 99, Avg Loss = 0.177519\n",
            "-- Epoch 4: PR AUC 0.705849, ROC AUC 0.716406, Validation Loss 0.220904\n",
            "-- Epoch 5, Step: 99, Avg Loss = 0.167661\n",
            "-- Epoch 5: PR AUC 0.712821, ROC AUC 0.723276, Validation Loss 0.219918\n",
            "-- Epoch 6, Step: 99, Avg Loss = 0.155327\n",
            "-- Epoch 6: PR AUC 0.712700, ROC AUC 0.724731, Validation Loss 0.222144\n",
            "-- Epoch 7, Step: 99, Avg Loss = 0.144185\n",
            "-- Epoch 7: PR AUC 0.692933, ROC AUC 0.709164, Validation Loss 0.234728\n",
            "-- Epoch 8, Step: 99, Avg Loss = 0.133943\n",
            "-- Epoch 8: PR AUC 0.700525, ROC AUC 0.717430, Validation Loss 0.234330\n",
            "-- Epoch 9, Step: 99, Avg Loss = 0.126748\n",
            "-- Epoch 9: PR AUC 0.689436, ROC AUC 0.710641, Validation Loss 0.244405\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Teacher Inference\n",
        "\n",
        "Once the `teacher_ft` task completed, we can then inference the entire training corpus to get teacher score on each training sample. The data set to be inferenced in this step is often refered to as **distillation data**, and it does not need to have human labels. That is why we can often leverage business logs in industrial scenairos, since we usually have plenty of business logs and sampling from these logs is much easier and cheaper than labeling by human judges.\n",
        "\n",
        "Here we will do the inference on the same 100K training data used in the above finetuning steps. The scale of this data is much smaller than what we typically have in industrial scenarios (where we can sample billions of logs), but as we will see later, this facilitates a fair comparison between `student_ft` and `student_kd`:"
      ],
      "metadata": {
        "id": "vp__hcF8dsC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task teacher_inf --val_batch_size 4096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5go8Z5z6K63k",
        "outputId": "8422da84-5eb0-4d47-ae6e-66e09a4b5bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments\n",
            "-- Argument: device                     -- cuda\n",
            "-- Argument: learning_rate              -- 0.0001\n",
            "-- Argument: load_dataset_py_path       -- load_dataset.py\n",
            "-- Argument: logfreq                    -- 100\n",
            "-- Argument: max_length                 -- 32\n",
            "-- Argument: max_length_ad              -- 24\n",
            "-- Argument: max_length_query           -- 9\n",
            "-- Argument: num_epochs                 -- 10\n",
            "-- Argument: output_dir                 -- output/teacher_inf\n",
            "-- Argument: student_pretrained         -- google/bert_uncased_L-2_H-128_A-2\n",
            "-- Argument: task                       -- teacher_inf\n",
            "-- Argument: teacher_pretrained         -- google/bert_uncased_L-4_H-256_A-4\n",
            "-- Argument: train_batch_size           -- 256\n",
            "-- Argument: val_batch_size             -- 4096\n",
            "Reusing dataset xglue (/root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08)\n",
            "100% 7/7 [00:00<00:00, 956.17it/s]\n",
            "Load labeled dataset.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-5595ab17a87c5d28.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-fada6ddeda8b2bf9.arrow\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Load Teacher model google/bert_uncased_L-4_H-256_A-4.\n",
            "Parameter 'function'=<function prepare_data_loader.<locals>.preprocess_function at 0x7fec5ca4def0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 98/98 [00:10<00:00,  9.59ba/s]\n",
            "100% 10/10 [00:01<00:00,  8.75ba/s]\n",
            "Load model from output/teacher_ft/model_best.pth\n",
            "Inference started.\n",
            "Write prediction results to output/teacher_inf/prediction.tsv.\n",
            "-- Predict 1000 samples, done.\n",
            "-- Predict 2000 samples, done.\n",
            "-- Predict 3000 samples, done.\n",
            "-- Predict 4000 samples, done.\n",
            "-- Predict 5000 samples, done.\n",
            "-- Predict 6000 samples, done.\n",
            "-- Predict 7000 samples, done.\n",
            "-- Predict 8000 samples, done.\n",
            "-- Predict 9000 samples, done.\n",
            "-- Predict 10000 samples, done.\n",
            "-- Predict 11000 samples, done.\n",
            "-- Predict 12000 samples, done.\n",
            "-- Predict 13000 samples, done.\n",
            "-- Predict 14000 samples, done.\n",
            "-- Predict 15000 samples, done.\n",
            "-- Predict 16000 samples, done.\n",
            "-- Predict 17000 samples, done.\n",
            "-- Predict 18000 samples, done.\n",
            "-- Predict 19000 samples, done.\n",
            "-- Predict 20000 samples, done.\n",
            "-- Predict 21000 samples, done.\n",
            "-- Predict 22000 samples, done.\n",
            "-- Predict 23000 samples, done.\n",
            "-- Predict 24000 samples, done.\n",
            "-- Predict 25000 samples, done.\n",
            "-- Predict 26000 samples, done.\n",
            "-- Predict 27000 samples, done.\n",
            "-- Predict 28000 samples, done.\n",
            "-- Predict 29000 samples, done.\n",
            "-- Predict 30000 samples, done.\n",
            "-- Predict 31000 samples, done.\n",
            "-- Predict 32000 samples, done.\n",
            "-- Predict 33000 samples, done.\n",
            "-- Predict 34000 samples, done.\n",
            "-- Predict 35000 samples, done.\n",
            "-- Predict 36000 samples, done.\n",
            "-- Predict 37000 samples, done.\n",
            "-- Predict 38000 samples, done.\n",
            "-- Predict 39000 samples, done.\n",
            "-- Predict 40000 samples, done.\n",
            "-- Predict 41000 samples, done.\n",
            "-- Predict 42000 samples, done.\n",
            "-- Predict 43000 samples, done.\n",
            "-- Predict 44000 samples, done.\n",
            "-- Predict 45000 samples, done.\n",
            "-- Predict 46000 samples, done.\n",
            "-- Predict 47000 samples, done.\n",
            "-- Predict 48000 samples, done.\n",
            "-- Predict 49000 samples, done.\n",
            "-- Predict 50000 samples, done.\n",
            "-- Predict 51000 samples, done.\n",
            "-- Predict 52000 samples, done.\n",
            "-- Predict 53000 samples, done.\n",
            "-- Predict 54000 samples, done.\n",
            "-- Predict 55000 samples, done.\n",
            "-- Predict 56000 samples, done.\n",
            "-- Predict 57000 samples, done.\n",
            "-- Predict 58000 samples, done.\n",
            "-- Predict 59000 samples, done.\n",
            "-- Predict 60000 samples, done.\n",
            "-- Predict 61000 samples, done.\n",
            "-- Predict 62000 samples, done.\n",
            "-- Predict 63000 samples, done.\n",
            "-- Predict 64000 samples, done.\n",
            "-- Predict 65000 samples, done.\n",
            "-- Predict 66000 samples, done.\n",
            "-- Predict 67000 samples, done.\n",
            "-- Predict 68000 samples, done.\n",
            "-- Predict 69000 samples, done.\n",
            "-- Predict 70000 samples, done.\n",
            "-- Predict 71000 samples, done.\n",
            "-- Predict 72000 samples, done.\n",
            "-- Predict 73000 samples, done.\n",
            "-- Predict 74000 samples, done.\n",
            "-- Predict 75000 samples, done.\n",
            "-- Predict 76000 samples, done.\n",
            "-- Predict 77000 samples, done.\n",
            "-- Predict 78000 samples, done.\n",
            "-- Predict 79000 samples, done.\n",
            "-- Predict 80000 samples, done.\n",
            "-- Predict 81000 samples, done.\n",
            "-- Predict 82000 samples, done.\n",
            "-- Predict 83000 samples, done.\n",
            "-- Predict 84000 samples, done.\n",
            "-- Predict 85000 samples, done.\n",
            "-- Predict 86000 samples, done.\n",
            "-- Predict 87000 samples, done.\n",
            "-- Predict 88000 samples, done.\n",
            "-- Predict 89000 samples, done.\n",
            "-- Predict 90000 samples, done.\n",
            "-- Predict 91000 samples, done.\n",
            "-- Predict 92000 samples, done.\n",
            "-- Predict 93000 samples, done.\n",
            "-- Predict 94000 samples, done.\n",
            "-- Predict 95000 samples, done.\n",
            "-- Predict 96000 samples, done.\n",
            "-- Predict 97000 samples, done.\n",
            "Inference completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This operation would output a prediction.tsv file under `output/teacher_inf`. We need to copy this file to `data/QADSM/`, since this is where `load_dataset.py` would try to load the inferenced data in the next step."
      ],
      "metadata": {
        "id": "I_wHOjxuxNOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('data/QADSM'):\n",
        "  os.makedirs('data/QADSM')\n",
        "!cp output/teacher_inf/prediction.tsv data/QADSM/prediction.tsv"
      ],
      "metadata": {
        "id": "tO9CzERNOdSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Distill Knowledge from Teacher to Student\n",
        "\n",
        "Finally it comes to the real distillation step! All we need to do is to run the code snippet once again with task `student_kd`. Note that this time we need to specify the relative path of `load_dataset.py` using the `load_dataset_py_path` parameter:"
      ],
      "metadata": {
        "id": "Kd-jjtD2eN9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task student_kd --train_batch_size 512 --val_batch_size 2048 --load_dataset_py_path kdd_2022_tutorial/load_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrmMnLYZPGlj",
        "outputId": "3a81d81f-7b06-4ec4-cacf-51a8253c34f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments\n",
            "-- Argument: device                     -- cuda\n",
            "-- Argument: learning_rate              -- 0.0001\n",
            "-- Argument: load_dataset_py_path       -- kdd_2022_tutorial/load_dataset.py\n",
            "-- Argument: logfreq                    -- 100\n",
            "-- Argument: max_length                 -- 32\n",
            "-- Argument: max_length_ad              -- 24\n",
            "-- Argument: max_length_query           -- 9\n",
            "-- Argument: num_epochs                 -- 10\n",
            "-- Argument: output_dir                 -- output/student_kd\n",
            "-- Argument: student_pretrained         -- google/bert_uncased_L-2_H-128_A-2\n",
            "-- Argument: task                       -- student_kd\n",
            "-- Argument: teacher_pretrained         -- google/bert_uncased_L-4_H-256_A-4\n",
            "-- Argument: train_batch_size           -- 512\n",
            "-- Argument: val_batch_size             -- 2048\n",
            "Reusing dataset xglue (/root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08)\n",
            "\r  0% 0/7 [00:00<?, ?it/s]\r100% 7/7 [00:00<00:00, 1020.80it/s]\n",
            "Reusing dataset load_dataset (/root/.cache/huggingface/datasets/load_dataset/qadsm/1.0.0/3e4ba7dc24c91e556ba03766349d9f351ba985164daf1bc3028d557f21da1bd7)\n",
            "Load unlabeled dataset using kdd_2022_tutorial/load_dataset.py.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/load_dataset/qadsm/1.0.0/3e4ba7dc24c91e556ba03766349d9f351ba985164daf1bc3028d557f21da1bd7/cache-c40ed7a8e7627061.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-fada6ddeda8b2bf9.arrow\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Load Student model (TwinBERT) google/bert_uncased_L-2_H-128_A-2.\n",
            "Parameter 'function'=<function prepare_data_loader.<locals>.preprocess_function_student at 0x7fe7ea3cd710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 98/98 [00:10<00:00,  9.60ba/s]\n",
            "100% 10/10 [00:01<00:00,  9.52ba/s]\n",
            "Training started.\n",
            "-- Epoch 0, Step: 99, Avg Loss = 0.073917\n",
            "-- Epoch 0: PR AUC 0.583300, ROC AUC 0.598758, Validation Loss 0.244563\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 1, Step: 99, Avg Loss = 0.049713\n",
            "-- Epoch 1: PR AUC 0.619012, ROC AUC 0.632784, Validation Loss 0.238311\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 2, Step: 99, Avg Loss = 0.035592\n",
            "-- Epoch 2: PR AUC 0.628193, ROC AUC 0.644251, Validation Loss 0.237992\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 3, Step: 99, Avg Loss = 0.028659\n",
            "-- Epoch 3: PR AUC 0.639187, ROC AUC 0.659132, Validation Loss 0.235447\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 4, Step: 99, Avg Loss = 0.024463\n",
            "-- Epoch 4: PR AUC 0.649346, ROC AUC 0.668371, Validation Loss 0.232446\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 5, Step: 99, Avg Loss = 0.021560\n",
            "-- Epoch 5: PR AUC 0.655915, ROC AUC 0.675013, Validation Loss 0.231598\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 6, Step: 99, Avg Loss = 0.019656\n",
            "-- Epoch 6: PR AUC 0.654873, ROC AUC 0.674631, Validation Loss 0.230785\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 7, Step: 99, Avg Loss = 0.017593\n",
            "-- Epoch 7: PR AUC 0.658487, ROC AUC 0.678702, Validation Loss 0.229103\n",
            "-- Best checkpoint so far!\n",
            "-- Epoch 8, Step: 99, Avg Loss = 0.016387\n",
            "-- Epoch 8: PR AUC 0.655840, ROC AUC 0.676510, Validation Loss 0.229728\n",
            "-- Epoch 9, Step: 99, Avg Loss = 0.015386\n",
            "-- Epoch 9: PR AUC 0.658807, ROC AUC 0.677519, Validation Loss 0.230085\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Now that all the major steps for knowledge distillaiton have completed, we want to know whether all these efforts have brought any real impact. To see this, let us run our script for the last time with task `eval`, which will load the best checkpoint under `teacher_ft`, `student_ft` and `student_kd` settings respectively and conduct evaluation on the same test data.\n",
        "\n",
        "The would print a table in the end of its log file, where the last row highlights the improvement by comparing metrics for `student_kd` against that of `student_ft`. As we can see, even though we experiment on such a small data set with barely no advanced training strategies nor hyper-parameter tuning, we could see a remarkable 3% AUC lift:"
      ],
      "metadata": {
        "id": "ScxHsPTNgYwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02MlBLRTggMj",
        "outputId": "58af3077-e027-422d-a381-cac6407cd329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments\n",
            "-- Argument: device                     -- cuda\n",
            "-- Argument: learning_rate              -- 0.0001\n",
            "-- Argument: load_dataset_py_path       -- load_dataset.py\n",
            "-- Argument: logfreq                    -- 100\n",
            "-- Argument: max_length                 -- 32\n",
            "-- Argument: max_length_ad              -- 24\n",
            "-- Argument: max_length_query           -- 9\n",
            "-- Argument: num_epochs                 -- 10\n",
            "-- Argument: output_dir                 -- output/eval\n",
            "-- Argument: student_pretrained         -- google/bert_uncased_L-2_H-128_A-2\n",
            "-- Argument: task                       -- eval\n",
            "-- Argument: teacher_pretrained         -- google/bert_uncased_L-4_H-256_A-4\n",
            "-- Argument: train_batch_size           -- 256\n",
            "-- Argument: val_batch_size             -- 256\n",
            "Reusing dataset xglue (/root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08)\n",
            "100% 7/7 [00:00<00:00, 1015.32it/s]\n",
            "Load labeled dataset.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/xglue/qadsm/1.0.0/8566eedecd9ab28e01c051c023dadf97bf408e5195f76b06aba70ebd4697ae08/cache-fada6ddeda8b2bf9.arrow\n",
            "Evaluate Task teacher_ft.\n",
            "-- Load model from output/teacher_ft/model_best.pth\n",
            "Parameter 'function'=<function prepare_data_loader.<locals>.preprocess_function at 0x7fb677874950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 10/10 [00:00<00:00, 10.46ba/s]\n",
            "Evaluate Task student_ft.\n",
            "-- Load model from output/student_ft/model_best.pth\n",
            "100% 10/10 [00:00<00:00, 10.19ba/s]\n",
            "Evaluate Task student_kd.\n",
            "-- Load model from output/student_kd/model_best.pth\n",
            "100% 10/10 [00:01<00:00,  9.60ba/s]\n",
            "Task          PR AUC   ROC AUC\n",
            "teacher_ft    0.7166    0.7280\n",
            "student_ft    0.6362    0.6504\n",
            "student_kd    0.6585    0.6787\n",
            "--------------------------------\n",
            "delta          3.50%     4.35%\n"
          ]
        }
      ]
    }
  ]
}